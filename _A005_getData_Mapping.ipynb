{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get mapping tables from different sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import pandas as pd\n",
    "\n",
    "# Custom modules\n",
    "import Tools\n",
    "\n",
    "# Hidden configurations\n",
    "from mySecrets import config_file, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_BUCKET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected.\n"
     ]
    }
   ],
   "source": [
    "# Connect to AWS S3 storage\n",
    "s3 = Tools.S3()\n",
    "s3.connect(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config File\n",
    "\n",
    "Extract mapping and configuration information from the config file and upload for easy access to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETF Overivew\n",
    "etf_info = pd.read_excel(config_file, sheet_name = \"ETF_Overview\", header = 1)\n",
    "etf_info = etf_info[['Security_ISIN', 'Security_Name', 'Security_Class', 'Fund_Company', 'Domicile', 'Type']]\n",
    "\n",
    "s3.uploadFile(\n",
    "    df = etf_info, \n",
    "    name = 'etf_info',\n",
    "    dir = 'config/mapping/',\n",
    "    bucket = AWS_BUCKET\n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info on equity securities\n",
    "equity_info = pd.read_excel(config_file, sheet_name = \"Equity_Overview\", header = 1)\n",
    "equity_info = equity_info[['Security_ISIN', 'Name', 'Sektor', 'Standort', 'Type', 'ISO3']]\n",
    "\n",
    "s3.uploadFile(\n",
    "    df = equity_info, \n",
    "    name = 'equity_info',\n",
    "    dir = 'config/mapping/',\n",
    "    bucket = AWS_BUCKET\n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping from ISO3 to Regions\n",
    "map_iso3 = pd.read_excel(config_file, sheet_name = \"Region\", header = 0)\n",
    "map_iso3 = map_iso3[['Region Name', 'Sub-region Name', 'Country or Area', 'ISO-alpha3 Code']]\n",
    "\n",
    "# Rename columns\n",
    "map_iso3.columns = ['region_name', 'region_sub_name', 'country_name', 'ISO3']\n",
    "\n",
    "s3.uploadFile(\n",
    "    df = map_iso3, \n",
    "    name = 'map_iso3',\n",
    "    dir = 'config/mapping/',\n",
    "    bucket = AWS_BUCKET\n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETF ISIN - Symbol\n",
    "\n",
    "etf_isin_symbol = pd.read_excel(config_file, sheet_name = \"ETF_Overview\", header = 1)\n",
    "etf_isin_symbol = etf_isin_symbol[['Security_ISIN', 'Sym_YahooFin', 'Ex_YahooFin', 'CCY_YahooFin']]\n",
    "\n",
    "s3.uploadFile(\n",
    "    df = etf_isin_symbol, \n",
    "    name = 'etf_isin_symbol',\n",
    "    dir = 'config/mapping/',\n",
    "    bucket = AWS_BUCKET\n",
    ")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLEIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ISIN to LEI\n",
    "# Data Source: https://www.gleif.org/en/lei-data/lei-mapping/download-isin-to-lei-relationship-files#\n",
    "# Data Size: 224 MB\n",
    "# Data TSV Size: 283 MB\n",
    "\n",
    "file_isin_to_lei = \"C:/Users/phili/OneDrive/GitHub/Portfolio_Management/mapping/isin-lei-20220308T080155/ISIN_LEI_20220308.csv\"\n",
    "\n",
    "isin_to_lei = pd.read_csv(file_isin_to_lei)\n",
    "\n",
    "isin_to_lei.to_csv(\"temp/isin_to_lei.tsv\", sep = \"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_upload = \"config/mapping/isin_to_lei.tsv\"\n",
    "filename_temp = \"temp/isin_to_lei.tsv\"\n",
    "\n",
    "s3.client.upload_file(\n",
    "    filename_temp, \n",
    "    AWS_BUCKET, \n",
    "    filename_upload\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEI information - Level 1 data\n",
    "# Data Source: https://www.gleif.org/en/lei-data/gleif-concatenated-file/download-the-concatenated-file\n",
    "# Data Source size: 4.3 GB\n",
    "# Data TSV select columns size: 161 MB\n",
    "\n",
    "# pd.read_xml  breaks after 10min with \"IO Encoder\" Error\n",
    "# ET.iterparse Takes 2min 21sec\n",
    "\n",
    "filename = \"C:/Users/phili/OneDrive/GitHub/Portfolio_Management/mapping/20220308-gleif-concatenated-file-lei2.xml\"\n",
    "\n",
    "# https://stackoverflow.com/questions/14924200/loading-huge-xml-files-and-dealing-with-memoryerror\n",
    "# Stream xml\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Stream file\n",
    "parser = ET.iterparse(filename)\n",
    "\n",
    "# Create empty dictionary to store information\n",
    "temp = []\n",
    "\n",
    "r = 0\n",
    "for event, element in parser:\n",
    "    # LEI defines new data entry\n",
    "    el = element.tag.split('}', 1)[1]\n",
    "    if el == 'LEI':\n",
    "        # Create new row\n",
    "        temp.append({})\n",
    "        r = r +  1\n",
    "\n",
    "    if r != 0:\n",
    "        temp[r-1][el] = element.text\n",
    "\n",
    "    # then clean up\n",
    "    element.clear()\n",
    "\n",
    "df = pd.DataFrame(temp)\n",
    "\n",
    "# Define relevant columns\n",
    "cols = ['LEI', 'LegalName', 'City', 'Country', 'EntityStatus']\n",
    "\n",
    "# Remove inactive entries? -> 4% are inactive\n",
    "\n",
    "# Save as tsv\n",
    "df[cols].to_csv(\"temp/LEI_info.tsv\", sep = \"\\t\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_upload = \"config/mapping/lei_info.tsv\"\n",
    "filename_temp = \"temp/lei_info.tsv\"\n",
    "\n",
    "s3.client.upload_file(\n",
    "    filename_temp, \n",
    "    AWS_BUCKET, \n",
    "    filename_upload\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "869cfd7152767b6c01cc3b1a533d38f34d102a29f7c510c98fad66d349226c64"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
